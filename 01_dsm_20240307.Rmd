---
title: "Density surface modelling"
params:
  seed_no: 16
  species_name: "impala"
  segdata_file: "segdata_impala.xlsx"
  obsdata_file: "obsdata_impala.xlsx"
  preddata_file: "preddata_impala_new.xlsx"
  convertion_km_m: !expr 1/1000
output:
  html_notebook:
    toc: yes
    toc_float: yes
    code_folding: hide
    theme: cosmo
  html_document:
    df_print: paged
  word_document:
    toc: yes
  pdf_document:
    toc: yes
editor_options: 
  markdown: 
    wrap: 72
---

```{r packages, message=FALSE}
# install.packages(c("dsm", "Distance", "distill", "rgdal",
#                    "maptools", "plyr", "tweedie"))
#### Load packages
library(tidyverse)
library(readxl)
library(mvtnorm)
# if(!"devtools" %in% rownames(installed.packages())) 
#   {install.packages("devtools")}
# devtools::install_github('david-borchers/LT2D')
library(LT2D)
library(dsm)
library(Distance)
library(distill)
# library(rgdal)
library(plyr)
# library(maptools)
library(tweedie)
```

```{r functions}
#### Load 2D distance functions
source("functions/com_hfunctions.R")
source("functions/com_pifunctions.R")
source("functions/com_likelihoodutilities.R")
source("functions/GoFy_mod.R") # custom GoFy function, modified by VLM 2022-11-11
source("functions/dsm_custom_functions.R")
```

In this notebook, we follow up on the distance sampling analyses in two dimensions, and we use the corrected detection function, taking into account the behavioral response of the animals, to fit a density surface model (DSM, Hedley & Buckland 2004, Miller et al. 2013) to the data. Indeed, DSMs follow a two-stage approach: after accounting for detectability via distance sampling methods, they then model distribution via a generalized additive model. Considering that the precision in density estimates may be largely affected by the spatial variation in the data (e.g., large variance in the encounter rate), the DSM is expected to produce an increase in the precision of the final estimates by modelling the spatial component.

**Analyses for `r params$species_name`**

# Data preparation

## Data import

First of all, we need to prepare the data for the following DSM analysis. We need three data.frames:

-   **segdata** holds the segment data (transects "chopped" into segments), with all the covariates
-   **obsdata** obsdata links the distance data (already used in the previous step of the analysis to fit the detection function) to the segments
-   **preddata** holds the prediction grid (which includes all the necessary covariates)

For detailed instructions on data preparation, and in particular for the **segdata**, you car refer to the above mentioned publications (Hedley & Buckland 2004, Miller et al. 2013), but also to <https://distancesampling.org>, and to the GitHub (<https://https://github.com/DistanceDevelopment/dsm/>) and Wiki (<https://https://github.com/DistanceDevelopment/dsm/wiki>) pages.

*Please note that when reading the file, we specify the column types: make sure to maintain the recommended order of the columns to avoid errors in the procedure; additional columns could of course be added, or the order changed, but then you will have to modify the `col_types` argument accordingly*

```{r segdata-loading}
#### Load dataset
segdata <- read_excel(paste("data/",params$segdata_file,sep=""),
                      # orignal file name: impala_segmenti_vlm_NEW.xlsx/imapla_segementi_vlm_NEW.xlsx 
                      #"data/impala_segmenti_vlm.xlsx",
                      #sheet="template_dataset",
                      col_types = c(rep("text", 2),
                                    rep("numeric", 3)))
head(segdata)
segdata$Effort <- segdata$Effort/1000 # effort now expressed in km
```

The `segdata` dataset (Excel file) should include the following columns (order matters):

-   `Transect.Label`: transect label (it could be a number or a letter)
-   `Sample.Label`: label that identifies the segment within a transect, it is composed by the transect label (`Transect.Label`), a separator (e.g., -), and a unique identifier of the segment within the transect
-   `x`: x-coord of the center of the segment
-   `y`: y-coord of the center of the segment
-   `Effort`: length of the segment, in m

and additional, optional columns can be added, to describe the segments in terms of environmental covariates (e.g., information on habitats, distance from water sources or from anthropic areas such as villages, altitude, ...). All optional fields can have empty cells (or *NA*) in the Excel file (although this will affect their use in the following modelling procedures). On the contrary, *NA*s are not admitted in the fields: `area`, `Transect.Label`, `Sample.Label`, `x`, `y`, `Effort`.

```{r obsdata-loading}
obsdata <- read_excel(paste("data/",params$obsdata_file,sep=""),
                      #sheet="template_dataset",
                      col_types = c("numeric",
                                    "text",
                                    rep("numeric", 3)))
head(obsdata)
obsdata$Effort <- obsdata$Effort/1000 # effort now expressed in km
```

The `obsdata` dataset (Excel file) should include the following columns (order matters):

-   `object`: a progressive number to identify each record (*i.e.*, 1,2,3,4,...) - this field connects the `obsdata` table to the data used in the previous analysis to fit the detection function (`data_trunc_SPECIES NAME`)
-   `Sample.Label`: label that identifies the segment within a transect, it is composed by the transect label (`Transect.Label`), a separator (e.g., -), and a unique identifier of the segment within the transect - this field connects the `obsdata` table to the `segdata` table
-   `x`: x-coord of the group of animals (optional)
-   `y`: y-coord of the group of animals (optional)
-   `Effort`: length of the segment, in m (optional).   
The `x` and `y` fields can have empty cells (or *NA*) in the Excel file. On the contrary, *NA*s are not admitted in the fields: `object`, `Sample.Label`, `Effort`.

For the observations, we have to consider the ones already used in the previous step of the analysis to fit the detection function. Thus, we load the corresponding *.RData* file and we link the two datasets (`obsdata` and `trunc_data`) using the `object` field to obtain the final `obsdata` object.

```{r obsdata-def}
# please note that data_trunc comes from the previous 2D distance sampling analysis
load(paste("output/data_trunc_",params$species_name,".RData",sep = ""))
left_join(obsdata, data_trunc, by = "object") %>% 
  dplyr::select(object:Effort, cluster_size, perp_dist) %>% 
  dplyr::rename(size = cluster_size, distance = perp_dist) -> obsdata

# include a column to identify the detection function object: here, we only have one detection function
# so the ddfobj colum has a fixed value of 1 and it is added to both the segments and the observation datasets
obsdata %>% 
  mutate(ddfobj = 1) -> obsdata
segdata %>% 
  mutate(ddfobj = 1) -> segdata

head(obsdata)
```

Please note that a column `ddfobj` has been added to identify the detection function used (in this case there is only one so it is a constant). The same column has been added to the `segdata` table as well.

```{r preddata-loading}
#### Load dataset
preddata <- read_excel(paste("data/", params$preddata_file, sep = ""),
                      col_types = rep("numeric", 4))
head(preddata)
preddata$area <- preddata$area/1000000
```

Finally, the `preddata` dataset (Excel file) should include at least the following columns: - `id`: a unique identifier for each grid cell in the study area - `x`: x-coord of the center of the cell - `y`: y-coord of the center of the cell - `area`: the area of each cell (in m$^2$) Please pay attention to the coordinates of the cells, that should be in the same reference system used for the other datasets, in particular the `segdata`. Additional columns including the values for environmental covariates can also be included.

## Detection function

To reconstruct the detection function for the following DSM, we consider the detection function object obtained in the previous phase and we derive a table with the estimated detection probability for each observation:

```{r detection-probs-table}
load(paste("output/fitVU_", params$species_name, ".RData", sep = ""))

est = fitVU

Nhat.yx=bias=NULL
b=est$b; hr=match.fun(est$hr); ystart=est$ystart; pi.x=match.fun(est$pi.x)
logphi=est$logphi; w=est$w
x = obsdata$distance
f.x=p.x.std=adbnTRUE=0
arrange(obsdata, distance) %>% 
  dplyr::select(object, distance) %>% 
  na.omit() -> gridx.df

p.xpifit=p.pi.x(gridx.df$distance,b,hr,ystart,pi.x,logphi,w)
mufit=integrate(f=p.pi.x,lower=0,upper=w,b=b,hr=hr
                  ,ystart=ystart,pi.x=pi.x,logphi=logphi,w=w)$value # phat media dei gruppi
f.xfit=p.xpifit/mufit
p.xfit=px(gridx.df$distance,b,hr,ystart,nint=nint)
ptot=integrate(f=px,lower=0,upper=w,b=b,hr=hr,ystart=ystart)$value
p.xfit.std=p.xfit/ptot
adbn=pi.x(gridx.df$distance,logphi,w)

bind_cols(p=p.xfit.std, gridx.df) -> fitted.p.df
bind_cols(p=p.xfit, gridx.df) -> fitted.p.df
fitted.p.df
```

Moreover, we need to store the information on detection probabilities in a *fake* detection function object, that will be called by the DSM functions later on.

```{r detection-function-fake}
# if we didn't use ds()...
# probs <- hr.model$ddf$fitted
# object.ids <- names(hr.model$ddf$fitted)

probs <- fitted.p.df$p
object.ids <- fitted.p.df$object

fake.ddf <- list()
fake.ddf$meta.data <- list()
# fake.ddf$meta.data$width <- max(mexdolphins$distdata$distance)
fake.ddf$meta.data$width <- max(fitted.p.df$distance)
fake.ddf$meta.data$left <- 0
fake.ddf$fitted <- probs
names(fake.ddf$fitted) <- object.ids
fake.ddf
```

# Model fitting

In order to fit the DSMs, first of all we prepare all the above mentioned data using a customized `make.data` function

```{r}
dat <- make.data_mod(response = "abundance.est", ddfobject = fitVU,
                     segdata, obsdata, group = FALSE,
                     convert.units = params$convertion_km_m,
                     availability = 1,
                     segment.area = segdata$Effort*max(fitted.p.df$distance)*2,
                     family = tw())
dat
```

We can now fit the `dsm` to these data:

```{r dsm-fit}
mod1<-dsm_mod(abundance.est ~ s(x,y,k=4), fake.ddf,
              segment.data, observation.data, dat = dat,
              # convert.units = 1,
              convert.units = params$convertion_km_m,
              group = FALSE)
summary(mod1)
```

# Prediction

To predict the abundance, we apply the model to the grid data, obtaining the overall abundance (number of individuals) and visualizing the smooth of the spatial coordinates:

```{r dsm-prediction}
# predict over a grid
mod1.pred <- predict(mod1, preddata, preddata$area)
# calculate the predicted abundance over the grid
print("Predicted abundance (no. individuals) over the grid:")
sum(mod1.pred)
limit <- summary(mod1.pred)[5]*1.5
sum(mod1.pred[mod1.pred<limit])
# plot the smooth
plot(mod1)
```

We now estimate the variance for the model:

```{r dsm-variance}
## --- Variance estimation for the chosen model ----
preddata.varprop.subset <- split(preddata, 1:nrow(preddata))
offset.varprop.subset <- as.list(rep(preddata$area[1],nrow(preddata)))
dsm.var.subset <- dsm_var_gam(dsm.obj=mod1, pred.data=preddata.varprop.subset, 
                              off.set = offset.varprop.subset, #seglen.varname = "Effort", 
                              type.pred = "response")
# summary(dsm.var.subset)
fName = "h.RE"
summary.dsm.var_mod(object = dsm.var.subset, detfunc = fitVU)
```

## Maps of abundance and uncertainty

Finally, we can plot the results to show the abundances within the study area, and the corresponding uncertainty map. The spatially explict abundance data are also saved to a .csv file (`r paste("DSMresults_", params$species_name, ".csv")`, in the output folder) for later use in a GIS software, if needed.

```{r dsm-maps}
## ---- prediction and plot Nhat and uncertainty ----
prediction.Nhat <- unlist(dsm.var.subset$pred)
prediction.CV <- sqrt(dsm.var.subset$pred.var)/unlist(dsm.var.subset$pred)
prediction.stdev <- sqrt(dsm.var.subset$pred.var)
pp <- mutate(as.data.frame(preddata), Nhat = prediction.Nhat)
pp.uncertainty <- mutate(pp, CV = prediction.CV, stdev = prediction.stdev)
# plot Nhat
pNhat <- ggplot(pp) +
  geom_tile(aes(x=x, y=y, fill=Nhat, width=400, height=400)) +
  scale_fill_distiller(palette = "YlGn", direction=2) +
  #                      values = c(seq(0, 60, 60))) +
  # scale_fill_distiller(palette = "YlGn", direction=2, trans = "log10", 
  #                      name = "log10(Similarity)") +
  labs(fill="Nhat") +
  theme_minimal() +
  coord_equal()
print(pNhat)


# plot SD
pSD <- ggplot(pp) +
  geom_tile(aes(x=x, y=y, fill=prediction.stdev, width=400, height=400)) +
  labs(fill="SD") +
  theme_minimal() +
  coord_equal()
print(pSD)

# pDens <- ggplot(pp) +
#   geom_tile(aes(x=x, y=y, fill=Nhat/0.160000, width=400, height=400)) +
#   scale_fill_distiller(palette = "YlGn", direction=2) +
#   #                      values = c(seq(0, 60, 60))) +
#   # scale_fill_distiller(palette = "YlGn", direction=2, trans = "log10", 
#   #                      name = "log10(Similarity)") +
#   labs(fill="density") +
#   theme_minimal() +
#   coord_equal()
# print(pDens)


## ---- save abundance map for GIS ----
write.csv(pp,paste("output/DSMresults_", params$species_name,
                   ".csv", sep = ""))

# pp.uncertainty <- read.csv(paste("output/DSMresults_",
#                                  params$species_name,
#                                  ".csv", sep = ""))
# sum(pp.uncertainty$Nhat)
```

# References

Hedley SL, Buckland ST. Spatial models for line transect sampling. Journal of Agricultural, Biological, and Environmental Statistics. 2004;9(2):181--199.\
Miller DL, Burt ML, Rexstad EA, Thomas L. Spatial models for distance sampling data: recent developments and future directions. Methods in Ecology and Evolution. 2013;4(11):1001--1010. doi: 10.1111/2041-210X.12105.
