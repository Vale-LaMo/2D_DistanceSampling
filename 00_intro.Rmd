---
title: "2D distance sampling"
params:
  seed_no: 16
  species_name: "impala"
  input_file: "impala_vlm.xlsx"
  trunc_perp_dist: 5
output:
  html_document:
    df_print: paged
---

```{r packages, message=FALSE}
#### Load packages
library(tidyverse)
library(readxl)
library(mvtnorm)
# if(!"devtools" %in% rownames(installed.packages())) 
#   {install.packages("devtools")}
# devtools::install_github('david-borchers/LT2D')
library(LT2D)
```

```{r functions}
#### Load 2D distance functions
source("functions/com_hfunctions.R")
source("functions/com_pifunctions.R")
source("functions/com_likelihoodutilities.R")
source("functions/GoFy_vlm.R") # custom GoFy function, modified by VLM 2022-11-11
source("functions/plotfit.x.red.R") # custom function, modified by VLM 2023-08-31, to have a red line instead of a grey one
```

In distance sampling surveys, the animals might avoid both the transects in the absence of observers, and the observers themselves.
To correct for the effect of the behavioral responses of the animals to either the transects or the observers, we can estimate density and abundance using line transect survey data with both the forward and perpendicular distances to the observers (2D distance sampling - R LT2D package, Borchers and Cox 2015), not just the perpendicular distance. This analysis approach was also applied and recommended by Elenga et al. (2020).

Here, we rely on the functions from LT2D package (https://github.com/david-borchers/LT2D), as partly revised and applied in Elenga et al. (2020) (https://github.com/cbonenfant/duikers-abundance). With respect to the latter, we made additional minor changes to the code. All the code and functions used are available on https://github.com/Vale-LaMo/2D_DistanceSampling

To perform the analyses, initial parameters (species name, input file, percentage used for distance data truncation) are set at the beginning of this notebook (they can be customized manually, or via "Knit with parameters" in the Knit menu)

# Detection function fitting: impala

## Data import

First of all, we need to import the data. The dataset (Excel file) should include the following columns (order matters):   

- `area`: surface of the study area, in km2
- `transect`: transect label (it could be a number or a letter)
- `transect_length`: length of the transect, in km
- `detected`: a field whose value is 1 in case of detection of a group of animals, 0 otherwise
- `object`: a progressive number to identify each record (*i.e.*, 1,2,3,4,...)
- `perp_dist`: perpendicular distance to the observer (only if the group has been detected)
- `forw_dist`: forward distance to the observer (only if the group has been detected)
- `cluster_size`: number of animals in the group (optional, only if the group has been detected)
- `obs_time`: date and time stamp (optional)
- `X_observer`: x-coord of the observer (optional)
- `Y_observer`: y-coord of the observer (optional)

All optional fields and the distance columns can have empty cells (or *NA*) in the Excel file. On the contrary, *NA*s are not admitted in the fields: `area`, `transect`, `transect_length`, `detected`, `object` (i.e., if the above order is respected, *NAs* are not admitted in the first 5 columns but can be present in the other ones). Values in the `forw_dist` column can also be negative.    
In this file, we include all transects, even those for which there were no detections (and in this case, the `detected` column will be 0).

*Please note that when reading the file, we specify the column types, that is why the order is important. Make sure to maintain the recommended order of the columns to avoid errors in the procedure; additional columns could of course be added, or the order changed, but then you will have to modify the `col_types` argument accordingly*

```{r data-loading}
#### Load dataset
data <- read_excel(paste("data/",params$input_file,sep=""), #sheet="template_dataset",
                   col_types = c(rep("numeric", 8),
                                 "date",
                                 "text", "text"))
```
*The previous messages simply warn us on the presence of some NAs in the columns with numeric data. We will deal with them later but please go back and check your data if you did not expect this to happen.*

The data should look as follows:
```{r data-preview}
# data$transect_length <- data$transect_length*1000
# data$area <- data$area*1000000
head(data)
```

## Data cleaning and truncation

Second, we clean the data: we remove the lines of the transects without any observation (`detected = 0`) and we also exclude records for which distances are missing.

```{r data-cleaning}
#### Dealing with NA and non-detections
data_clean <- 
  data %>% 
  filter(detected != 0, # we only include actual observations in the dataset used to fit the detection function
         perp_dist != "NA", # we remove lines with NA distances
         forw_dist != "NA")
data_clean$forw_dist <- abs(data_clean$forw_dist) # we make sure all distances are positive (see Discussion for details)
```

We now select the truncation distances. For this purpose, we considere a standard truncation of 5% of the data and we produce histograms and boxplots to identify outliers. The percentage of data can be changed by modifying the parameters of this notebook.

```{r truncation-perpendicular-distances}
hist(data_clean$perp_dist, main = "", xlab = "Perpendicular distance (m)")
boxplot(data_clean$perp_dist, ylab = "Perpendicular distance (m)")

no_data <- round(params$trunc_perp_dist*length(data_clean$perp_dist)/100,0) # no. data to be deleted
threshold <- sort(data_clean$perp_dist, decreasing = TRUE)[no_data+1] # threshold
data_trunc <- 
  data_clean %>% 
  filter(perp_dist <= threshold)
save(data_trunc, file = paste("output/data_trunc_", params$species_name,".RData", sep = ""), compress = FALSE)
```

The first histogram shows we have a very long tail for perpendicular distances, and the boxplot confirms that we have potential outliers. Using a truncation distance of `r params$trunc_perp_dist`%, we remove `r no_data` records.   

We also produce the histogram and the boxplot for the forward distances:

```{r truncation-forward-distances}
hist(data_trunc$forw_dist, main = "", xlab = "Forward distance (m)")
boxplot(data_trunc$forw_dist, ylab = "Forward distance (m)")
```


If outliers are detected in the forward distances, they can be removed with the following code (optional):

```{r}
# ystart = max(data_trunc$forw_dist) # change this to the desired truncation distance if necessary, e.g. 
ystart = 150
data_trunc <- 
  data_trunc %>% 
  filter(forw_dist <= ystart)
```


## Model fitting and estimation of groups in the surveyed region

We now fit 2D distance sampling model using multiple initial value to avoid local *minima* in the deviance (Elenga et al 2020) and we obtain the estimates of the detection probabilities (p-hat) and the number of groups (N) with bootstrap confidence intervals:

```{r model-fitting}
seed_no <- set.seed(params$seed_no)
#### Model fitting
y = data_trunc$forw_dist
x = data_trunc$perp_dist
hr = h.RE # h.yTRE not compatible with pi.sigmoI
# these functions work: h.RE, h.IP, h.SS, h.okamura
pi.x = pi.sigmo
# functions tested and working with h.RE: pi.sigmo, pi.CHN, pi.TN
ystart = ceiling(max(y))
w = ceiling(max(x))
length.b = 2
debug=FALSE

 FIT=list(); dev=NULL
 for (m in 1:10) {
   set.seed(params$seed_no)
   pars = rnorm(4, c(0.25,0.25,-4,-1), 3)
   set.seed(params$seed_no)
   tmp0 <- tryCatch.W.E (
     fityx(y,x,pars[1:length.b],
           hr,ystart,pi.x,pars[(length.b+1):length(pars)],w,control=list(),
           hessian=TRUE,corrFlag=0.7,debug=FALSE)
   )
   fit = NA
   if(! "error" %in% class(tmp0$value)) {
     fit <- tmp0$value
     fit$vcov <-  matrix(Matrix::nearPD(fit$vcov)$mat,4,4)
   }
   FIT[[m]] = fit
   if(is.na(fit[1])) dev=c(dev, 1e12) else dev = c(dev, fit$val)
 }
 fitVU = FIT[[which.min(dev)]]
 tabVU = matrix(NA,2,3)
 if(is.na(fitVU[1])) tabVU = matrix(NA,2,3) else {
   # set.seed(10)
   tmp1 <- tryCatch.W.E (boot(fitVU))
 if(! "error" %in% class(tmp1$value))  tabVU=tmp1$value
 }
# tabVU
if(!is.numeric(unlist(tabVU))) print("error, change seed_no to any random number")
save(fitVU, file = paste("output/fitVU_", params$species_name, ".RData", sep = ""), compress = FALSE)
```

The following figure shows the actual distribution of animals (continuous black line), the "observed" detection function (bold red line) and the "corrected" detection function (dashed black line), that takes into account the behavioural response

```{r echo=FALSE, eval=FALSE}
tryCatch.W.E(plotfit.x(x[x<=w],fitVU,nclass=20,nint=100));rug(x[x<=w])
# see https://github.com/david-borchers/LT2D/blob/master/inst/FitsForPaper.r
```

```{r}
tryCatch.W.E(plotfit.x.red(x[x<=w],fitVU,nclass=20,nint=100));rug(x[x<=w])
# see https://github.com/david-borchers/LT2D/blob/master/inst/FitsForPaper.r
# the original plotfit.x function has been modified to customize the colors
```


We now perform checks on the model, to verify the goodness of fit:

```{r gof-tests}
fName = "h1"
GoFx(fitVU,plot=TRUE)$pvals
plotfit.y(y[x<=w & y<=ystart],x,fitVU,nclass=20);rug(x=y[x<=w])
plotfit.smoothfy(fitVU,nclass=32);rug(x=y[x<=w])
GoFy_vlm(fitVU,plot=TRUE)$pvals

plotfit.smoothfy(fitVU,xmax=199)
``` 
and we finally summarise the results on the **detection probability** and the **number of groups** in the surveyed region:

```{r detection-prob}
(LT2D::phatModels(modList = FIT[1], n=length(na.omit(data_trunc$cluster_size))) -> stats_df_groups)
```
Indeed, number of groups in the surveyed area is estimated by dividing the number of observed groups (`n`) for the detection probability. The following table shows the estimated number of groups and the density per square km:

```{r groups}
length(na.omit(data_trunc$cluster_size))/(phatInterval(fitVU))[1] -> no_groups
names(no_groups) <- "no_groups"

data %>% 
  mutate(transetto = factor(transect)) %>%
  dplyr::group_by(transetto) %>% 
  dplyr::summarise(no_groups_transect = sum(detected),
                   transect_length = mean(transect_length)) %>% 
  mutate(encounter_rate = no_groups_transect/transect_length) -> res
# res

(2*(w/1000)*sum(res$transect_length)) -> surveyed_area # here, the truncation distance is divided by 1000, to express the density in km2
(no_groups/surveyed_area) -> dens_groups_km2 
names(dens_groups_km2) <- "dens_groups_km2"
cbind(no_groups, dens_groups_km2)
```


## Cluster size stats and estimated number of individuals with CV

In order to estimate the number of individuals, we now consider the cluster size data, summarizing them and calculating the cluster size standard deviation:

```{r}
data_clustersize <- 
  data %>% 
  filter(detected != 0,
         perp_dist != "NA",
         forw_dist != "NA",
         perp_dist <= w,
         forw_dist <= ystart)
data_clustersize$forw_dist <- abs(data_clustersize$forw_dist)
print("Cluster size base stats:")
summary(data_clustersize$cluster_size)
print("Cluster size standard deviation:")
sd(data_clustersize$cluster_size)
```

The estimated abundance of individual animals (`abund_survey_individuals`) is obtained by multiplying the estimated number of groups (`no_groups`) for the mean cluster size.   
Then, we can estimate the overall coefficient of variation of this estimate using the Delta method. According to this approximation, when two or more components are multiplied together, the squared CVs add. In this case the components of the formula to estimate the abundance (or density) are the encounter rate, the detection function and the cluster size. 

```{r}
no_groups*mean(data_clustersize$cluster_size) -> abund_survey_individuals # estimated abundance, individuals
data.frame(abund_survey_individuals[1],abund_survey_individuals/surveyed_area) -> df
names(df) <- c("no_individuals","dens_individuals_km2")
df

cv_encounterrate <- (sd(res$encounter_rate)/mean(res$encounter_rate))
cv_detfunc <- (phatInterval(fitVU)[2])
cv_clustersize <- (sd(data_clustersize$cluster_size)/mean(data_clustersize$cluster_size))

cv_tot <- sqrt(cv_detfunc^2 + cv_clustersize^2 + cv_encounterrate^2)

component = c("Encounter rate", "Cluster size", "Detection function", "Abundance")
CV = c(cv_encounterrate, cv_clustersize, cv_detfunc[[1]], cv_tot[[1]])
data.frame(component, CV)

```
The table with the details of the CVs allows to identify potential issues, e.g. components that strongly affect the overall coefficient of variation.   

Finally, the essential results of the survey:


```{r}
cat(" Number of oservations","\t",":","\t",dim(data_trunc)[1],"\n",
    "\n",
    "Perpendicular distance range (m)","\t",":","\t",min(data_trunc$perp_dist)," - ",max(data_trunc$perp_dist),"\n",
    "Forward distance range (m)","\t",":","\t", min(data_trunc$forw_dist)," - ",max(data_trunc$forw_dist),"\n",
    "\n",
    "Model","\t",":","\t",fName[1],"\n",
    "AIC","\t",":","\t",fitVU$AIC,"\n",
    "\n",
    "No. transects","\t",":","\t",dim(res)[1],"\n",
    "Effort (km)","\t",":","\t",sum(res$transect_length),"\n",
    "\n",
    "Surveyed area (km2)","\t",":","\t",surveyed_area)

stats_df_groups$NhatLower*mean(data_clustersize$cluster_size) -> ind_min
stats_df_groups$NhatUpper*mean(data_clustersize$cluster_size) -> ind_max

rows = c("Average p", "N groups", "N individuals")
Estimate = c(stats_df_groups$phat, stats_df_groups$Nhat, abund_survey_individuals[[1]])
Lower = c(stats_df_groups$lower.bound, stats_df_groups$NhatLower, ind_min)
Upper = c(stats_df_groups$upper.bound, stats_df_groups$NhatUpper, ind_max)
data.frame(rows,Estimate,Lower,Upper)
      
```


