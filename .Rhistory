library(readxl)
library(mvtnorm)
# if(!"devtools" %in% rownames(installed.packages()))
#   {install.packages("devtools")}
# devtools::install_github('david-borchers/LT2D')
library(LT2D)
library(dsm)
library(Distance)
library(distill)
# library(rgdal)
library(plyr)
# library(maptools)
library(tweedie)
#### Load 2D distance functions
source("functions/com_hfunctions.R")
source("functions/com_pifunctions.R")
source("functions/com_likelihoodutilities.R")
source("functions/GoFy_mod.R") # custom GoFy function, modified by VLM 2022-11-11
source("functions/dsm_custom_functions.R")
#### Load dataset
segdata <- read_excel(paste("data/",params$segdata_file,sep=""),
# orignal file name: impala_segmenti_vlm_NEW.xlsx/imapla_segementi_vlm_NEW.xlsx
#"data/impala_segmenti_vlm.xlsx",
#sheet="template_dataset",
col_types = c(rep("text", 2),
rep("numeric", 3)))
head(segdata)
segdata$Effort <- segdata$Effort # effort now expressed in m
obsdata <- read_excel(paste("data/",params$obsdata_file,sep=""),
#sheet="template_dataset",
col_types = c("numeric",
"text",
rep("numeric", 3)))
head(obsdata)
obsdata$Effort <- obsdata$Effort # effort now expressed in m
# please note that data_trunc comes from the previous 2D distance sampling analysis
load(paste("output/data_trunc_",params$species_name,".RData",sep = ""))
left_join(obsdata, data_trunc, by = "object") %>%
dplyr::select(object:Effort, cluster_size, perp_dist) %>%
dplyr::rename(size = cluster_size, distance = perp_dist) -> obsdata
# include a column to identify the detection function object: here, we only have one detection function
# so the ddfobj colum has a fixed value of 1 and it is added to both the segments and the observation datasets
obsdata %>%
mutate(ddfobj = 1) -> obsdata
segdata %>%
mutate(ddfobj = 1) -> segdata
head(obsdata)
#### Load dataset
preddata <- read_excel(paste("data/", params$preddata_file, sep = ""),
col_types = rep("numeric", 4))
head(preddata)
preddata$area <- preddata$area
load(paste("output/fitVU_", params$species_name, ".RData", sep = ""))
est = fitVU
Nhat.yx=bias=NULL
b=est$b; hr=match.fun(est$hr); ystart=est$ystart; pi.x=match.fun(est$pi.x)
logphi=est$logphi; w=est$w
x = obsdata$distance
f.x=p.x.std=adbnTRUE=0
arrange(obsdata, distance) %>%
dplyr::select(object, distance) %>%
na.omit() -> gridx.df
p.xpifit=p.pi.x(gridx.df$distance,b,hr,ystart,pi.x,logphi,w)
mufit=integrate(f=p.pi.x,lower=0,upper=w,b=b,hr=hr
,ystart=ystart,pi.x=pi.x,logphi=logphi,w=w)$value # phat media dei gruppi
f.xfit=p.xpifit/mufit
p.xfit=px(gridx.df$distance,b,hr,ystart,nint=nint)
ptot=integrate(f=px,lower=0,upper=w,b=b,hr=hr,ystart=ystart)$value
p.xfit.std=p.xfit/ptot
adbn=pi.x(gridx.df$distance,logphi,w)
bind_cols(p=p.xfit.std, gridx.df) -> fitted.p.df
bind_cols(p=p.xfit, gridx.df) -> fitted.p.df
fitted.p.df
# if we didn't use ds()...
# probs <- hr.model$ddf$fitted
# object.ids <- names(hr.model$ddf$fitted)
probs <- fitted.p.df$p
object.ids <- fitted.p.df$object
fake.ddf <- list()
fake.ddf$meta.data <- list()
# fake.ddf$meta.data$width <- max(mexdolphins$distdata$distance)
fake.ddf$meta.data$width <- max(fitted.p.df$distance)
fake.ddf$meta.data$left <- 0
fake.ddf$fitted <- probs
names(fake.ddf$fitted) <- object.ids
fake.ddf
dat <- make.data_mod(response = "abundance.est", ddfobject = fitVU,
segdata, obsdata, group = FALSE,
convert.units = params$conversion_km_m,
availability = 1,
segment.area = segdata$Effort*max(fitted.p.df$distance)*2,
family = tw())
dat
mod1<-dsm_mod(abundance.est ~ s(x,y,k=4), fake.ddf,
segment.data, observation.data, dat = dat,
# convert.units = 1,
convert.units = params$conversion_km_m,
group = FALSE)
summary(mod1)
# predict over a grid
mod1.pred <- predict(mod1, preddata, preddata$area)
# mod1.pred <- predict(mod1, preddata, preddata$area*1000)
# calculate the predicted abundance over the grid
print("Predicted abundance (no. individuals) over the grid:")
sum(mod1.pred)
limit <- summary(mod1.pred)[5]*1.5
sum(mod1.pred[mod1.pred<limit])
# plot the smooth
plot(mod1)
## --- Variance estimation for the chosen model ----
preddata.varprop.subset <- split(preddata, 1:nrow(preddata))
offset.varprop.subset <- as.list(rep(preddata$area[1],nrow(preddata)))
dsm.var.subset <- dsm_var_gam(dsm.obj=mod1, pred.data=preddata.varprop.subset,
off.set = offset.varprop.subset, #seglen.varname = "Effort",
type.pred = "response")
# summary(dsm.var.subset)
fName = "h.RE"
summary.dsm.var_mod(object = dsm.var.subset, detfunc = fitVU)
## ---- prediction and plot Nhat and uncertainty ----
prediction.Nhat <- unlist(dsm.var.subset$pred)
prediction.CV <- sqrt(dsm.var.subset$pred.var)/unlist(dsm.var.subset$pred)
prediction.stdev <- sqrt(dsm.var.subset$pred.var)
pp <- mutate(as.data.frame(preddata), Nhat = prediction.Nhat)
pp.uncertainty <- mutate(pp, CV = prediction.CV, stdev = prediction.stdev)
# plot Nhat
pNhat <- ggplot(pp) +
geom_tile(aes(x=x, y=y, fill=Nhat, width=400, height=400)) +
scale_fill_distiller(palette = "YlGn", direction=2) +
#                      values = c(seq(0, 60, 60))) +
# scale_fill_distiller(palette = "YlGn", direction=2, trans = "log10",
#                      name = "log10(Similarity)") +
labs(fill="Nhat") +
theme_minimal() +
coord_equal()
print(pNhat)
# plot SD
pSD <- ggplot(pp) +
geom_tile(aes(x=x, y=y, fill=prediction.stdev, width=400, height=400)) +
labs(fill="SD") +
theme_minimal() +
coord_equal()
print(pSD)
# pDens <- ggplot(pp) +
#   geom_tile(aes(x=x, y=y, fill=Nhat/0.160000, width=400, height=400)) +
#   scale_fill_distiller(palette = "YlGn", direction=2) +
#   #                      values = c(seq(0, 60, 60))) +
#   # scale_fill_distiller(palette = "YlGn", direction=2, trans = "log10",
#   #                      name = "log10(Similarity)") +
#   labs(fill="density") +
#   theme_minimal() +
#   coord_equal()
# print(pDens)
## ---- save abundance map for GIS ----
write.csv(pp,paste("output/DSMresults_", params$species_name,
".csv", sep = ""))
# pp.uncertainty <- read.csv(paste("output/DSMresults_",
#                                  params$species_name,
#                                  ".csv", sep = ""))
# sum(pp.uncertainty$Nhat)
preddata$area[1]
#### Load packages
library(tidyverse)
library(readxl)
library(mvtnorm)
# if(!"devtools" %in% rownames(installed.packages()))
#   {install.packages("devtools")}
# devtools::install_github('david-borchers/LT2D')
library(LT2D)
library(Distance)
#### Load 2D distance functions
source("functions/com_hfunctions.R")
source("functions/com_pifunctions.R")
source("functions/com_likelihoodutilities.R")
source("functions/GoFy_mod.R") # custom GoFy function, modified by VLM 2022-11-11
source("functions/plotfit.x.red.R") # custom function, modified by VLM 2023-08-31, to have a red line instead of a grey one
#### Load dataset
data <- read_excel(paste("data/",params$input_file,sep=""), #sheet="template_dataset",
col_types = c(rep("numeric", 8),
"date",
"text", "text"))
# data$transect_length <- data$transect_length*1000
# data$area <- data$area*1000000
head(data)
#### Dealing with NA and non-detections
data_clean <-
data %>%
filter(detected != 0, # we only include actual observations in the dataset used to fit the detection function
perp_dist != "NA", # we remove lines with NA distances
forw_dist != "NA")
data_clean$forw_dist <- abs(data_clean$forw_dist) # we make sure all distances are positive (see Discussion for details)
write.csv(data_clean, paste("data_clean_",params$species_name,".csv", sep=""))
write.csv(data_clean, paste("output/data_clean_",params$species_name,".csv", sep=""))
#### Load packages
library(tidyverse)
library(readxl)
library(mvtnorm)
# if(!"devtools" %in% rownames(installed.packages()))
#   {install.packages("devtools")}
# devtools::install_github('david-borchers/LT2D')
library(LT2D)
library(Distance)
#### Load 2D distance functions
source("functions/com_hfunctions.R")
source("functions/com_pifunctions.R")
source("functions/com_likelihoodutilities.R")
source("functions/GoFy_mod.R") # custom GoFy function, modified by VLM 2022-11-11
source("functions/plotfit.x.red.R") # custom function, modified by VLM 2023-08-31, to have a red line instead of a grey one
#### Load dataset
data <- read_excel(paste("data/",params$input_file,sep=""), #sheet="template_dataset",
col_types = c(rep("numeric", 8),
"date",
"text", "text"))
# data$transect_length <- data$transect_length*1000
# data$area <- data$area*1000000
head(data)
#### Dealing with NA and non-detections
data_clean <-
data %>%
filter(detected != 0, # we only include actual observations in the dataset used to fit the detection function
perp_dist != "NA", # we remove lines with NA distances
forw_dist != "NA")
data_clean$forw_dist <- abs(data_clean$forw_dist) # we make sure all distances are positive (see Discussion for details)
write.csv(data_clean, paste("output/data_clean_",params$species_name,".csv", sep=""))
#### Load packages
library(tidyverse)
library(readxl)
library(mvtnorm)
# if(!"devtools" %in% rownames(installed.packages()))
#   {install.packages("devtools")}
# devtools::install_github('david-borchers/LT2D')
library(LT2D)
library(Distance)
#### Load 2D distance functions
source("functions/com_hfunctions.R")
source("functions/com_pifunctions.R")
source("functions/com_likelihoodutilities.R")
source("functions/GoFy_mod.R") # custom GoFy function, modified by VLM 2022-11-11
source("functions/plotfit.x.red.R") # custom function, modified by VLM 2023-08-31, to have a red line instead of a grey one
#### Load dataset
data <- read.csv("SAMPLE.csv")
#### Load dataset
data <- read.csv("data/SAMPLE.csv")
head(data)
colnames(data) <- c("perp_dist", "forw_dist", "samplesize", "cluster_size")
head(data)
data %>%
mutate(detected = 1) -> data
head(data)
#### Dealing with NA and non-detections
data_clean <-
data %>%
filter(detected != 0, # we only include actual observations in the dataset used to fit the detection function
perp_dist != "NA", # we remove lines with NA distances
forw_dist != "NA")
data_clean$forw_dist <- abs(data_clean$forw_dist) # we make sure all distances are positive (see Discussion for details)
plot(data_clean$perp_dist, data_clean$forw_dist,
xlim=c(0,max(data_clean$perp_dist)),
ylim=c(0,max(data_clean$forw_dist)),
xlab = "Perpedincular distance",
ylab = "Forward distance")
par(mfrow = c(1,2))
hist(data_clean$perp_dist, main = "", xlab = "Perpendicular distance (m)")
boxplot(data_clean$perp_dist, ylab = "Perpendicular distance (m)")
no_data <- round(params$trunc_perp_dist_perc*length(data_clean$perp_dist)/100,0) # no. data to be deleted
params$trunc_perp_dist_perc
params$trunc_perp_dist_perc <- 5
params$trunc_perp_dist_perc
no_data <- round(params$trunc_perp_dist_perc*length(data_clean$perp_dist)/100,0) # no. data to be deleted
no_data
threshold <- sort(data_clean$perp_dist, decreasing = TRUE)[no_data+1] # threshold
data_trunc <-
data_clean %>%
filter(perp_dist <= threshold)
data_trunc
par(mfrow = c(1,2))
hist(data_trunc$forw_dist, main = "", xlab = "Forward distance (m)")
boxplot(data_trunc$forw_dist, ylab = "Forward distance (m)")
# ystart = max(data_trunc$forw_dist) # change this to the desired truncation distance if necessary, e.g.
ystart = params$trunc_forw_dist_m
data_trunc <-
data_trunc %>%
filter(forw_dist <= ystart)
#### Model fitting
y = data_trunc$forw_dist
x = data_trunc$perp_dist
hr = params$h.function # h.yTRE not compatible with pi.sigmoI
hr
params$h.function <- "h.RE"
hr = params$h.function # h.yTRE not compatible with pi.sigmoI
hr
params$pi.function <- "pi.sigmo"
# these functions work: h.RE, h.IP, h.SS, h.okamura
pi.x = params$pi.function # perpendicular distance function used
# functions tested and working with h.RE: pi.sigmo, pi.CHN, pi.TN
ystart = ceiling(max(y))
w = ceiling(max(x))
params$n_hpars <- 2
params$n_pipars <- 2
length.b = params$n_hpars # pars for h function
length.logphi = params$n_pipars # pars for pi function
length.pars = length.b + length.logphi
debug=FALSE
FIT=list(); AICvalues=NULL
for (m in 1:params$n_models) {
set.seed(m)
pars = rnorm(length.pars, # tot no. pars
params$starting_values, params$sd)
set.seed(m)
tmp0 <- tryCatch.W.E (
fityx(y,x,pars[1:length.b],
hr,ystart,pi.x,
pars[(length.b+1):length(pars)],w,
control=list(),
hessian=TRUE,corrFlag=0.7,debug=FALSE)
)
fit = NA
if(! "error" %in% class(tmp0$value)) {
fit <- tmp0$value
fit$vcov <-  matrix(Matrix::nearPD(fit$vcov)$mat,length.pars,length.pars)
}
FIT[[m]] = fit
# if(is.na(fit[1])) dev=c(dev, 1e12) else dev = c(dev, fit$AIC)
# with the funciton used in this analyses, we add the constraint that the pi.x pars should be negative
# to maintain the sigmoid shape
if(params$pi.function == "pi.sigmo" & params$h.function == "h.RE") {
if(!is.na(fit[1])) {
if(any(is.nan(fit$corr)) | any(fit$par[3:4] > 0)) {
AICvalues=c(AICvalues, 1e12)
} else {
AICvalues=c(AICvalues, fit$AIC)
}
} else {
AICvalues=c(AICvalues, 1e12)
}
} else {
if(!is.na(fit[1])) {
if(any(is.nan(fit$corr))) {
# if(all(fit$b > 0)) {
AICvalues=c(AICvalues, 1e12)
} else {
AICvalues=c(AICvalues, fit$AIC)
}
} else {
AICvalues=c(AICvalues, 1e12)
}
}
}
params$n_models <-  200
FIT=list(); AICvalues=NULL
for (m in 1:params$n_models) {
set.seed(m)
pars = rnorm(length.pars, # tot no. pars
params$starting_values, params$sd)
set.seed(m)
tmp0 <- tryCatch.W.E (
fityx(y,x,pars[1:length.b],
hr,ystart,pi.x,
pars[(length.b+1):length(pars)],w,
control=list(),
hessian=TRUE,corrFlag=0.7,debug=FALSE)
)
fit = NA
if(! "error" %in% class(tmp0$value)) {
fit <- tmp0$value
fit$vcov <-  matrix(Matrix::nearPD(fit$vcov)$mat,length.pars,length.pars)
}
FIT[[m]] = fit
# if(is.na(fit[1])) dev=c(dev, 1e12) else dev = c(dev, fit$AIC)
# with the funciton used in this analyses, we add the constraint that the pi.x pars should be negative
# to maintain the sigmoid shape
if(params$pi.function == "pi.sigmo" & params$h.function == "h.RE") {
if(!is.na(fit[1])) {
if(any(is.nan(fit$corr)) | any(fit$par[3:4] > 0)) {
AICvalues=c(AICvalues, 1e12)
} else {
AICvalues=c(AICvalues, fit$AIC)
}
} else {
AICvalues=c(AICvalues, 1e12)
}
} else {
if(!is.na(fit[1])) {
if(any(is.nan(fit$corr))) {
# if(all(fit$b > 0)) {
AICvalues=c(AICvalues, 1e12)
} else {
AICvalues=c(AICvalues, fit$AIC)
}
} else {
AICvalues=c(AICvalues, 1e12)
}
}
}
params$starting_values <-  c(0.25,0.25,-4,-1)
FIT=list(); AICvalues=NULL
for (m in 1:params$n_models) {
set.seed(m)
pars = rnorm(length.pars, # tot no. pars
params$starting_values, params$sd)
set.seed(m)
tmp0 <- tryCatch.W.E (
fityx(y,x,pars[1:length.b],
hr,ystart,pi.x,
pars[(length.b+1):length(pars)],w,
control=list(),
hessian=TRUE,corrFlag=0.7,debug=FALSE)
)
fit = NA
if(! "error" %in% class(tmp0$value)) {
fit <- tmp0$value
fit$vcov <-  matrix(Matrix::nearPD(fit$vcov)$mat,length.pars,length.pars)
}
FIT[[m]] = fit
# if(is.na(fit[1])) dev=c(dev, 1e12) else dev = c(dev, fit$AIC)
# with the funciton used in this analyses, we add the constraint that the pi.x pars should be negative
# to maintain the sigmoid shape
if(params$pi.function == "pi.sigmo" & params$h.function == "h.RE") {
if(!is.na(fit[1])) {
if(any(is.nan(fit$corr)) | any(fit$par[3:4] > 0)) {
AICvalues=c(AICvalues, 1e12)
} else {
AICvalues=c(AICvalues, fit$AIC)
}
} else {
AICvalues=c(AICvalues, 1e12)
}
} else {
if(!is.na(fit[1])) {
if(any(is.nan(fit$corr))) {
# if(all(fit$b > 0)) {
AICvalues=c(AICvalues, 1e12)
} else {
AICvalues=c(AICvalues, fit$AIC)
}
} else {
AICvalues=c(AICvalues, 1e12)
}
}
}
params$sd <-  6
FIT=list(); AICvalues=NULL
for (m in 1:params$n_models) {
set.seed(m)
pars = rnorm(length.pars, # tot no. pars
params$starting_values, params$sd)
set.seed(m)
tmp0 <- tryCatch.W.E (
fityx(y,x,pars[1:length.b],
hr,ystart,pi.x,
pars[(length.b+1):length(pars)],w,
control=list(),
hessian=TRUE,corrFlag=0.7,debug=FALSE)
)
fit = NA
if(! "error" %in% class(tmp0$value)) {
fit <- tmp0$value
fit$vcov <-  matrix(Matrix::nearPD(fit$vcov)$mat,length.pars,length.pars)
}
FIT[[m]] = fit
# if(is.na(fit[1])) dev=c(dev, 1e12) else dev = c(dev, fit$AIC)
# with the funciton used in this analyses, we add the constraint that the pi.x pars should be negative
# to maintain the sigmoid shape
if(params$pi.function == "pi.sigmo" & params$h.function == "h.RE") {
if(!is.na(fit[1])) {
if(any(is.nan(fit$corr)) | any(fit$par[3:4] > 0)) {
AICvalues=c(AICvalues, 1e12)
} else {
AICvalues=c(AICvalues, fit$AIC)
}
} else {
AICvalues=c(AICvalues, 1e12)
}
} else {
if(!is.na(fit[1])) {
if(any(is.nan(fit$corr))) {
# if(all(fit$b > 0)) {
AICvalues=c(AICvalues, 1e12)
} else {
AICvalues=c(AICvalues, fit$AIC)
}
} else {
AICvalues=c(AICvalues, 1e12)
}
}
}
data.frame(m = 1:params$n_models, modAIC = AICvalues) -> df.AIC
df.AIC %>%
arrange(modAIC) %>%
filter(modAIC <= min(df.AIC$modAIC) + 2) -> tab.AIC
tab.AIC
CV.phat.values <- vector("numeric", length(tab.AIC$m))
for (i in 1:length(tab.AIC$m)) {
fName = params$h.function
CV.phat.values[i] <- phatModels(list(FIT[[tab.AIC$m[i]]]))$CV.phat
# LT2D::phatModels(modList = list(FIT[tab.AIC$m[i]]))$CV.phat
}
(tab.AIC %>%
mutate(CV.phat = CV.phat.values) %>%
filter(CV.phat == min(CV.phat)) -> best_mod)
fitVU = FIT[[best_mod$m]]
plotfit.x.red(x[x<=w],fitVU,nclass=20,nint=100);rug(x[x<=w])
fName = params$h.function
# GoF for perpendicular distances
GoFx(fitVU,plot=TRUE)$pvals
# GoF for forward distances
fName = params$pi.function
GoFy_mod(fitVU,plot=TRUE)$pvals
# plotfit.smoothfy(fitVU,nclass=32);rug(x=y[x<=w])
# plotfit.y(y[x<=w & y<=ystart],x,fitVU,nclass=20);rug(x=y[x<=w])
plotfit.smoothfy(fitVU,xmax=199)
(LT2D::phatModels(modList = list(fitVU), # same as fitVU
n=length(na.omit(data_trunc$cluster_size))) -> stats_df_groups)
