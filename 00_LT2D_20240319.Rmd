---
title: "2D distance sampling"
params:
  species_name: impala
  input_file: distdata_impala.xlsx
  trunc_perp_dist_perc: 5
  trunc_forw_dist_m: 150
  h.function: h.RE
  n_hpars: 2
  pi.function: pi.sigmo
  n_pipars: 2
  starting_values: !expr c(0.25,0.25,-4,-1)
  sd: 6
  n_models: 200
output:
  html_notebook:
    toc: yes
    toc_float: yes
    code_folding: hide
    theme: cosmo
  html_document:
    df_print: paged
  word_document:
    toc: yes
  pdf_document:
    toc: yes
---

```{r packages, message=FALSE}
#### Load packages
library(tidyverse)
library(readxl)
library(mvtnorm)
# if(!"devtools" %in% rownames(installed.packages())) 
#   {install.packages("devtools")}
# devtools::install_github('david-borchers/LT2D')
library(LT2D)
library(Distance)
```

```{r functions}
#### Load 2D distance functions
source("functions/com_hfunctions.R")
source("functions/com_pifunctions.R")
source("functions/com_likelihoodutilities.R")
source("functions/GoFy_mod.R") # custom GoFy function, modified by VLM 2022-11-11
source("functions/plotfit.x.red.R") # custom function, modified by VLM 2023-08-31, to have a red line instead of a grey one
```

**Analyses for `r params$species_name`**

# Introduction and settings

In distance sampling surveys, the animals might avoid both the transects in the absence of observers, and the observers themselves.
To correct for the effect of the behavioral responses of the animals to either the transects or the observers, we can estimate density and abundance using line transect survey data with both the forward and perpendicular distances to the observers (2D distance sampling - R LT2D package, Borchers and Cox 2017), not just the perpendicular distance. This analysis approach was also applied and recommended by Elenga et al. (2020).

Here, we rely on the functions from LT2D package (https://github.com/david-borchers/LT2D), as partly revised and applied in Elenga et al. (2020) (https://github.com/cbonenfant/duikers-abundance). With respect to the latter, we made additional minor changes to the code. The code, data and functions used are available on https://github.com/Vale-LaMo/2D_DistanceSampling

**To perform the analyses, initial parameters (species name, input file, percentage used for perpendicular distance data truncation, forward truncation distance in meters, etc.) are set at the beginning of this notebook (they can be customized manually, or via "Knit with parameters" in the Knit menu - except for the starting values that can only be customized manually, but normally they should not be edited - see below).**
For the parameters `trunc_perp_dist_perc` and `trunc_forw_dist_m`, we recommend setting them respectively at 5, and at value >= than the largest forward distance. Then you can run the analyses and stop at the *Data cleaning and truncation* section to check the plots and eventually change these initial values.   
On the contrary, the parameters `h.function` and `pi.function`, and the corresponding number of parameters (`n_hpars` and `n_pipars`) are included in the header, but we recommend not editing them (unless you are an advanced user). The same applies for the `starting_values` and the `sd`. All these parameters concern the function used to model the radial detection function and the density of animals vs. distance from the transect.
The last parameter (`n_models`) defines the number of models that are fitted with different starting values, and we recommend to set it to a large number (e.g., 100 or 200).

**Please also note that we assume that the folder in which this .Rmd file is stored includes the subfolders named *data* and *output*. The first one should contain the input data, while the second one must be created as an empty folder - output of the DS analyses will be saved there for (optional) subsequent analyses. The subfolder *functions* is also essential since it contains the customized functions used for the analyses. The whole structure can however be recreated effortlessly by forking and then cloning the GitHub repository on your local machine.** (see the README for detailed instructions)

# Detection function fitting

## Data import

First of all, we need to import the data. The dataset (Excel file) should include the following columns (order matters):   

- `area`: surface of the study area, in km2
- `transect`: transect label (it could be a number or a letter)
- `transect_length`: length of the transect, in km
- `detected`: a field whose value is 1 in case of detection of a group of animals, 0 otherwise
- `object`: a progressive number to identify each record (*i.e.*, 1,2,3,4,...)
- `perp_dist`: perpendicular distance to the observer (only if the group has been detected), in meters
- `forw_dist`: forward distance to the observer (only if the group has been detected), in meters
- `cluster_size`: number of animals in the group (optional, only if the group has been detected)
- `obs_time`: date and time stamp (optional)
- `X_observer`: x-coord of the observer (optional)
- `Y_observer`: y-coord of the observer (optional)

All optional fields and the distance columns can have empty cells (or *NA*) in the Excel file. On the contrary, *NA*s are not admitted in the fields: `area`, `transect`, `transect_length`, `detected`, `object` (i.e., if the above order is respected, *NAs* are not admitted in the first 5 columns but can be present in the other ones). Values in the `forw_dist` column can also be negative.    
In this file, we include all transects, even those for which there were no detections (and in this case, the `detected` column will be 0).

*Please note that when reading the file, we specify the column types, that is why the order is important. Make sure to maintain the recommended order of the columns to avoid errors in the procedure; additional columns could of course be added, or the order changed, but then you will have to modify the `col_types` argument accordingly*

```{r data-loading}
#### Load dataset
data <- read_excel(paste("data/",params$input_file,sep=""), #sheet="template_dataset",
                   col_types = c(rep("numeric", 8),
                                 "date",
                                 "text", "text"))
```
*The previous messages simply warn us on the presence of some NAs in the columns with numeric data. We will deal with them later but please go back and check your data if you did not expect this to happen.*

The data should look as follows:
```{r data-preview}
# data$transect_length <- data$transect_length*1000
# data$area <- data$area*1000000
head(data)
```

## Data cleaning and truncation

Second, we clean the data: we remove the lines of the transects without any observation (`detected = 0`) and we also exclude records for which distances are missing.

```{r data-cleaning}
#### Dealing with NA and non-detections
data_clean <- 
  data %>% 
  filter(detected != 0, # we only include actual observations in the dataset used to fit the detection function
         perp_dist != "NA", # we remove lines with NA distances
         forw_dist != "NA")
data_clean$forw_dist <- abs(data_clean$forw_dist) # we make sure all distances are positive (see Discussion for details)
write.csv(data_clean, paste("output/data_clean_",params$species_name,".csv", sep=""))
```

The following plot shows the distribution of forward distances with respect to perpendicular distances:

```{r plot-avoidance}
plot(data_clean$perp_dist, data_clean$forw_dist,
     xlim=c(0,max(data_clean$perp_dist)),
     ylim=c(0,max(data_clean$forw_dist)),
     xlab = "Perpedincular distance",
     ylab = "Forward distance")
```


We now select the truncation distances. For this purpose, we produce histograms and boxplots to identify outliers:

```{r truncation-perpendicular-distances}
par(mfrow = c(1,2))
hist(data_clean$perp_dist, main = "", xlab = "Perpendicular distance (m)")
boxplot(data_clean$perp_dist, ylab = "Perpendicular distance (m)")

no_data <- round(params$trunc_perp_dist_perc*length(data_clean$perp_dist)/100,0) # no. data to be deleted
threshold <- sort(data_clean$perp_dist, decreasing = TRUE)[no_data+1] # threshold
data_trunc <- 
  data_clean %>% 
  filter(perp_dist <= threshold)
```

By applying a standard truncation distance of `r params$trunc_perp_dist_perc`%, we remove `r no_data` record(s).

*Please note that the percentage of data can be changed by modifying the parameters of this notebook (or via the "Knit with parameters" option in the Knit menu).*

We also produce the histogram and the boxplot for the forward distances:

```{r truncation-forward-distances}
par(mfrow = c(1,2))
hist(data_trunc$forw_dist, main = "", xlab = "Forward distance (m)")
boxplot(data_trunc$forw_dist, ylab = "Forward distance (m)")
```


If outliers are detected in the forward distances, they can be removed by setting a custom `trunc_forw_dist_m` parameter value. If truncation is not necessary, just set the `trunc_forw_dist_m` to value >= than the largest forward distance.

```{r outliers-forward-distances}
# ystart = max(data_trunc$forw_dist) # change this to the desired truncation distance if necessary, e.g.
ystart = params$trunc_forw_dist_m
data_trunc <- 
  data_trunc %>% 
  filter(forw_dist <= ystart)
```

When all truncations have been applied, the truncated data are saved to a `data_trunc` .RData file, to be used in the following (optional) DSM analyses.

```{r data_trunc-save}
save(data_trunc, file = paste("output/data_trunc_", params$species_name,".RData", sep = ""), compress = FALSE)
```


## Model fitting and estimation of the number of groups

We now fit 2D distance sampling model using multiple initial values to avoid local *minima* in the deviance (Elenga et al. 2020).   
As in Elenga et al. (2020), we model the detection function in two dimensions using a radial exponential hazard risk (\(h_{HB}\) under the notation of Borchers & Cox 2017), thereby making the same approximation as the half-normal detection function that is commonly found to describe the detection process in 1D. That is, we use the `h.RE` function of Elenga et al. (2020) for modeling the decay in detection rate with radial distance, and the `pi.sigmo` function of Elenga et al. (2020) for modeling the change in animal density with perpendicular distance to a line-transect (i.e., the behavioural response). See also Elenga et al. (2020), or the files "functions/com_hfunctions.R" and "functions/com_pifunctions.R" for alternative functions.   
Please note that the functions used are declared in the header of the document, together with the number of parameters that characterize each of them (different functions may have different number of parameters, e.g., 3 params for `h.yTRE`).   
`r params$n_models` (number set via the parameter `n_models`) are fitted using parameter values extracted from a random distribution, with the initial values for the mean (`starting_values`) and the sd (`sd`) declared in the header. The length of the `starting_values` depends on the number of parameters that characterize the adopted functions (check them if you change function type).

```{r model-fitting}
#### Model fitting
y = data_trunc$forw_dist
x = data_trunc$perp_dist
hr = params$h.function # h.yTRE not compatible with pi.sigmoI
# these functions work: h.RE, h.IP, h.SS, h.okamura
pi.x = params$pi.function # perpendicular distance function used
# functions tested and working with h.RE: pi.sigmo, pi.CHN, pi.TN
ystart = ceiling(max(y))
w = ceiling(max(x))
length.b = params$n_hpars # pars for h function
length.logphi = params$n_pipars # pars for pi function
length.pars = length.b + length.logphi
debug=FALSE

 FIT=list(); AICvalues=NULL
 for (m in 1:params$n_models) {
   set.seed(m)
   pars = rnorm(length.pars, # tot no. pars 
                params$starting_values, params$sd) 
   set.seed(m)
   tmp0 <- tryCatch.W.E (
     fityx(y,x,pars[1:length.b],
           hr,ystart,pi.x,
           pars[(length.b+1):length(pars)],w,
           control=list(),
           hessian=TRUE,corrFlag=0.7,debug=FALSE)
   )
   fit = NA
   if(! "error" %in% class(tmp0$value)) {
     fit <- tmp0$value
     fit$vcov <-  matrix(Matrix::nearPD(fit$vcov)$mat,length.pars,length.pars)
   }
   FIT[[m]] = fit
   # if(is.na(fit[1])) dev=c(dev, 1e12) else dev = c(dev, fit$AIC)
   
   # with the funciton used in this analyses, we add the constraint that the pi.x pars should be negative
   # to maintain the sigmoid shape
   if(params$pi.function == "pi.sigmo" & params$h.function == "h.RE") {
     if(!is.na(fit[1])) {
       if(any(is.nan(fit$corr)) | any(fit$par[3:4] > 0)) {
         AICvalues=c(AICvalues, 1e12)
       } else {
         AICvalues=c(AICvalues, fit$AIC)
       }
     } else {
       AICvalues=c(AICvalues, 1e12)
     }
   } else {
     if(!is.na(fit[1])) {
       if(any(is.nan(fit$corr))) {
         # if(all(fit$b > 0)) {
         AICvalues=c(AICvalues, 1e12)
       } else {
         AICvalues=c(AICvalues, fit$AIC)
       }
     } else {
       AICvalues=c(AICvalues, 1e12)
     }
   }
 }
#  fitVU = FIT[[which.min(dev)]]
#  tabVU = matrix(NA,2,3)
#  if(is.na(fitVU[1])) tabVU = matrix(NA,2,3) else {
#    # set.seed(10)
#    tmp1 <- tryCatch.W.E (boot(fitVU))
#  if(! "error" %in% class(tmp1$value))  tabVU=tmp1$value
#  }
# # tabVU # the CIs for the average p and the N of groups are generated by bootstrap
# if(!is.numeric(unlist(tabVU))) print("error!")
# save(fitVU, file = paste("output/fitVU_", params$species_name, ".RData", sep = ""), compress = FALSE)
save(FIT, file = paste("output/FIT_", params$species_name, ".RData", sep = ""), compress = FALSE)
```

```{r best-models, include=FALSE}
data.frame(m = 1:params$n_models, modAIC = AICvalues) -> df.AIC
df.AIC %>% 
  arrange(modAIC) %>% 
  filter(modAIC <= min(df.AIC$modAIC) + 2) -> tab.AIC
tab.AIC

# # tryCatch.W.E(plotfit.x.red(x[x<=w],fitVU,nclass=20,nint=100));rug(x[x<=w])
# # see https://github.com/david-borchers/LT2D/blob/master/inst/FitsForPaper.r
# # the original plotfit.x function has been modified to customize the colors
# # tryCatch.W.E(plotfit.x(x[x<=w],fitVU,nclass=20,nint=100));rug(x[x<=w]) # greyscale
# for (i in 1:length(tab.AIC$m)) {
#   plotfit.x.red(x[x<=w],FIT[[tab.AIC$m[i]]],nclass=20,nint=100);rug(x[x<=w])
# }
```

The models with the parameter values that provide the best fit are identified and we finally select the model with the lowest CV, and we plot it. The figure shows the actual distribution of animals (continuous black line), the "observed" detection function (bold red line) and the "corrected" detection function (dashed black line), that takes into account the behavioural response

```{r}
CV.phat.values <- vector("numeric", length(tab.AIC$m))
for (i in 1:length(tab.AIC$m)) {
  fName = params$h.function
  CV.phat.values[i] <- phatModels(list(FIT[[tab.AIC$m[i]]]))$CV.phat
  # LT2D::phatModels(modList = list(FIT[tab.AIC$m[i]]))$CV.phat
}
(tab.AIC %>%
  mutate(CV.phat = CV.phat.values) %>%
  filter(CV.phat == min(CV.phat)) -> best_mod)
fitVU = FIT[[best_mod$m]]
save(fitVU, file = paste("output/fitVU_", params$species_name, ".RData", sep = ""), compress = FALSE)
```

```{r plot-df-red-best-model, eval=TRUE}
# tryCatch.W.E(plotfit.x.red(x[x<=w],fitVU,nclass=20,nint=100));rug(x[x<=w])
# see https://github.com/david-borchers/LT2D/blob/master/inst/FitsForPaper.r
# the original plotfit.x function has been modified to customize the colors
plotfit.x.red(x[x<=w],fitVU,nclass=20,nint=100);rug(x[x<=w])
```

We now perform checks on the model, to verify the goodness of fit in the perpendicular dimension (Kolmogarov-Smirnov and Cramer-von Mises p-values are also reported):

```{r gof-tests-perp}
fName = params$h.function
# GoF for perpendicular distances
GoFx(fitVU,plot=TRUE)$pvals
``` 

We also verify the goodness of fit in the forward dimension

```{r}
# GoF for forward distances
fName = params$pi.function
GoFy_mod(fitVU,plot=TRUE)$pvals
# plotfit.smoothfy(fitVU,nclass=32);rug(x=y[x<=w])
# plotfit.y(y[x<=w & y<=ystart],x,fitVU,nclass=20);rug(x=y[x<=w])
plotfit.smoothfy(fitVU,xmax=199)
```

Finally, we summarise the results on the **detection probability** and the **number of groups** in the surveyed region:

```{r detection-prob}
(LT2D::phatModels(modList = list(fitVU), # same as fitVU
                 n=length(na.omit(data_trunc$cluster_size))) -> stats_df_groups)
# LT2D::phatModels(modList = list(FIT[76]))
```
Indeed, number of groups in the surveyed area is estimated by dividing the number of observed groups (`n`) for the detection probability. The following table shows the estimated number of groups and the density per square km:

```{r groups}
length(na.omit(data_trunc$cluster_size))/(phatInterval(fitVU))[1] -> no_groups
names(no_groups) <- "no_groups"

data %>% 
  mutate(transetto = factor(transect)) %>%
  dplyr::group_by(transetto) %>% 
  dplyr::summarise(no_groups_transect = sum(detected),
                   transect_length = mean(transect_length)) %>% 
  mutate(encounter_rate = no_groups_transect/transect_length) -> res
# res

(2*(w/1000)*sum(res$transect_length)) -> surveyed_area # here, the truncation distance is divided by 1000, to express the density in km2
(no_groups/surveyed_area) -> dens_groups_km2 
names(dens_groups_km2) <- "dens_groups_km2"
cbind(no_groups, dens_groups_km2)
```


## Cluster size stats and estimated number of individuals with CV

In order to estimate the number of individuals, we now consider the cluster size data, summarizing them and calculating the cluster size standard deviation:

```{r cluster-size}
data_clustersize <- 
  data %>% 
  filter(detected != 0,
         perp_dist != "NA",
         forw_dist != "NA",
         perp_dist <= w,
         forw_dist <= ystart)
data_clustersize$forw_dist <- abs(data_clustersize$forw_dist)
print("Cluster size base stats:")
summary(data_clustersize$cluster_size)
print("Cluster size standard deviation:")
sd(data_clustersize$cluster_size)
```

The estimated abundance of individual animals (`abund_survey_individuals`) is obtained by multiplying the estimated number of groups (`no_groups`) for the mean cluster size (obviously, if all groups are made of 1 individual only, the estimates of the number of groups and animals are equal).

```{r individuals}
no_groups*mean(data_clustersize$cluster_size) -> abund_survey_individuals # estimated abundance, individuals
data.frame(abund_survey_individuals[1],abund_survey_individuals/surveyed_area) -> df
names(df) <- c("no_individuals","dens_individuals_km2")
df
```

Then, we can estimate the overall coefficient of variation of this estimate using the Delta method. According to this approximation, when two or more components are multiplied together, the squared CVs add. In this case the components of the formula to estimate the abundance (or density) are the encounter rate, the detection function and the cluster size. 

```{r CVs}
sum(res$no_groups_transect)/sum(res$transect_length) -> ER
se.encounterrate <- sqrt(varn(res$transect_length, nvec=res$no_groups_transect, type = "R2"))
cv_encounterrate <- sqrt(varn(res$transect_length, nvec=res$no_groups_transect, type = "R2"))/mean(res$encounter_rate) # based on Fewster et al. 2009
cv_detfunc <- (phatInterval(fitVU)[2])

sum(data_clustersize$cluster_size)/length(data_clustersize$cluster_size) -> S
mean.clustersize <- mean(data_clustersize$cluster_size)
se.clustersize <- sqrt(var(data_clustersize$cluster_size)/length(data_clustersize$cluster_size))
cv_clustersize <- se.clustersize/mean.clustersize
# the estimate of cv_clustersize is based on the same calculations performed in the dht function of the Distance package

cv_tot <- sqrt(cv_detfunc^2 + cv_clustersize^2 + cv_encounterrate^2)

component = c("Encounter rate", "Cluster size", "Detection function", "Abundance")
CV = c(cv_encounterrate, cv_clustersize, cv_detfunc[[1]], cv_tot[[1]])
sd = c(se.encounterrate,
       se.clustersize,
       (cv_detfunc*phatInterval(fitVU)[1])[[1]],
       (cv_tot*abund_survey_individuals)[[1]])
data.frame(component, CV = round(CV,3), sd = round(sd, 3))
```

The table with the details of the CVs allows to identify potential issues, e.g. components that strongly affect the overall coefficient of variation.   

# Results - summary

Finally, the essential results of the survey:

```{r main-results}
stats <- list(dim(data_trunc)[1],
           paste(min(data_trunc$perp_dist)," - ",max(data_trunc$perp_dist)),
           paste(min(data_trunc$forw_dist)," - ",max(data_trunc$forw_dist)),
           paste(params$h.function,"/",params$pi.function),
           fitVU$AIC,
           dim(res)[1],
           sum(res$transect_length),
           surveyed_area)
names(stats) <- c("Number of oservations", "Perpendicular distance range (m)",
                      "Forward distance range (m)", "Model",
                      "AIC", "Number of transects",
                      "Effort (km)", "Surveyed area (km2)")
as.data.frame(do.call(rbind, stats)) -> statistics
colnames(statistics) <- NULL
statistics

D <- (abund_survey_individuals[[1]]/surveyed_area)
varD <- (cv_tot^2) * (D^2)
log(1+(varD/D^2)) -> var_logD
C <- exp(1.96*sqrt(var_logD))
D_min <- D/C[[1]]
D_max <- D*C[[1]]
ind_min <- D_min*surveyed_area
ind_max <- D_max*surveyed_area
# based on Buckland et al. 1993, Ch. 3, pp. 88-89

# rows = c("Average p", "N groups", "N individuals")
Estimate = c(stats_df_groups$phat,
             stats_df_groups$Nhat,
             abund_survey_individuals[[1]])
Lower = c(stats_df_groups$lower.bound, stats_df_groups$NhatLower, ind_min)
Upper = c(stats_df_groups$upper.bound, stats_df_groups$NhatUpper, ind_max)
data.frame(Estimate = round(Estimate,3),
           Lower = round(Lower,3),
           Upper = round(Upper,3)) -> results
row.names(results) <- c("Average p", "N groups", "N individuals")
results
```


# Comparison with Conventional Distance Sampling

The results of the 2D distance sampling are now compared to those from Conventional Distance Sampling (CDS). We start with the original data and make sure they align with the requirements of the `ds` function - the data frame must include the following fields:   
*(please note that we limit the estimate of the number of individual to the surveyed area, for comparison with the results of 2D distance sampling)*

+ Region.Label - identifier of regions: in this case there is only one region and set to 'Study area' **required field**
+ Area - size of the study region (hectares): in square kilometers in this case
+ Sample.Label - line transect identifier **required field**
+ Effort - length of the line transects (km)  **required field**
+ object - unique identifier for each detected animal or group
+ distance - perpendicular distance (metres) to each detection **required field**
+ Study.Area - this is the name of the study

```{r cds}
data %>% 
  mutate(Region.Label = "Study area",
         Area = surveyed_area) %>% 
  rename(#Area = area,
         Sample.Label = transect,
         Effort = transect_length,
         distance = perp_dist,
         size = cluster_size) -> data_cds
head(data_cds)
conversion.factor <- convert_units("meter", "kilometer", "Square kilometer")
cds.hr <- ds(data=data_cds, key="hr", adjustment=NULL,
             convert_units=conversion.factor,
             dht_group = FALSE, # group size must be taken into account
             truncation = threshold)
plot(cds.hr, main="Hazard rate model, Conventional Distance Sampling")
# summary(cds.hr)
```

FARE TABELLINA FINALE O PLOT CON PUNTI E BARRE DI ERRORE CON STIME OTTENUTE CON 1D, 2D, DSM

```{r comparison-table}
Estimate = c(cds.hr$dht$individuals$N$Estimate, abund_survey_individuals[[1]])
se = c(cds.hr$dht$individuals$N$se, sd[4])
cv = c(cds.hr$dht$individuals$N$cv, CV[4])
Lower = c(cds.hr$dht$individuals$N$lcl, ind_min)
Upper = c(cds.hr$dht$individuals$N$ucl, ind_max)
data.frame(Estimate = round(Estimate,3),
           se = round(se, 3),
           cv = round(cv, 3),
           Lower = round(Lower,3),
           Upper = round(Upper,3)) -> comparison
row.names(comparison) <- c("CDS", "2D-DS")
comparison
```
```{r comparison-plot}
library(ggplot2)
ggplot(comparison, aes(x=row.names(comparison), y=Estimate)) +
    geom_bar(position=position_dodge(.9), colour="black", stat="identity", fill = "white") +
    geom_errorbar(position=position_dodge(.9), width=.25, aes(ymin=Lower, ymax=Upper)) +
    xlab("") + ylab("Estimated N")
```


# References

Borchers DL, Cox MJ (2017) Distance sampling detection functions: 2D or not 2D? Biometrics 73(2):593-602. https://doi.org/10.1111/biom.12581    
Elenga G, Bonenfant C, Péron G (2020) Distance sampling of duikers in the rainforest: Dealing with transect avoidance. PLOS ONE 15(10): e0240049. https://doi.org/10.1371/journal.pone.0240049
Fewster, R.M., S.T. Buckland, K.P. Burnham, D.L. Borchers, P.E. Jupp, J.L. Laake and L. Thomas. 2009. Estimating the encounter rate variance in distance sampling. Biometrics 65: 225-236.

